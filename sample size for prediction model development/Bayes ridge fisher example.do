*** CODE FOR THE PRE-ECLAMPSIA EXAMPLE IN SECTION 3 OF RILEY ET AL. (submitted)
*** ref: "Riley RD et al. A general sample size framework for developing or updating a predictive algorithm: with application to clinical prediction models"


pmsampsize, type(b) cstatistic(0.759) prevalence(0.68) p(10)
* suggests min of 456 needed (335 for overall risk)


******* START HERE & RUN ALL CODE AT ONCE FROM HERE TO END
*** this code is for implementing the Fisher's decompoisition approximation for Bayesian logistic regression with ridge shrinkage priors
*** this avoids the fully simulation based approach, and so is considerably faster (but less accurate in small N) 
*** it essentially covers steps 6 to 10 of the sample size calculation (see Fig 3 in paper)  
*** the data setup phase has already taken place (using steps 1 to 5 in Fig 3 in the paper)
*** - here we simply read in these pre-generated development and evaluation synthetic datasets 


qui {
timer clear 
clear all

timer on 1


* create frame for development data
* this dev data is large synthetic data pre-generated by us, following steps 1 to 5 in the paper 
* it contains all the candidate predictors on both unstandardised and standardised scales
* and simulated outcome values based on the assumed reference model (see Fig 3 in paper)
* later we take a sample of size N from this data for the sample size investigation
capture frame create dev
frame dev: use "/YourLocation/RR_dev.dta", clear

* create frame for evaluation data
* this eval data is the large target population for evaluating model performance 
* (independent of the development dataset)
* it is synthetic data pre-generated by us, following steps 1 to 4 in the paper

capture frame create eval
frame eval: use "/YourLocation/RR_eval.dta", clear

 set seed 66
*sample 100000, count
drop if _n>100000

frame change dev

* For the Fisher's approximation, we need to specify a vector of logistic reg parameter 'estimates' and their variance-covariance matrix for a given N
* For the parameter 'estimates': we set these at the parameters values in the reference model;
* we can do this manually, but because we have a very large development dataset
* that was generated using the reference model parameters, we can simply fit a logistic model on that data to get very close to the ref model
* - then we use the stored estimates as a Y vector for the Fisher's approach

qui logistic outcome ageyrssd ga_diag1sd hist2sd hist3sd pcr1sd serurea11sd pcsd sbpsd trt_ahsd trt_mgso4sd, coef

* coefficient estimates (i.e., ref model parameter values) from logistic model now set to be our new data for Fisher's approach
mat estimates = e(b)
mat colnames estimates = y1 y2 y3 y4 y5 y6 y7 y8 y9 y10 y11
frame create estimates 
frame estimates: svmat estimates, n(col)

* for the var-cov matrix, we want to separate this into total sample size and the unit information matrix 
* again, using the fitted logistic model in the large data, we work out the inverse of the unit inf matrix 
mat invunit = e(V)*e(N)

* now we can define any sample size of interest (we consider various in the paper)
* and derive the var-cov matrix we anticipate
local n = 1500
* local n = 456
* local n = 335
 * local n = 75
* define new e(V)
mat varcov_new =  invunit /  `n'

* finally, define a separate frame of data with the beta values 
* and store a matrix that corresponds to the var-cov matrix of the sample size of interest
frame change estimates
svmat varcov_new, names(varcov)



********************************************************************

* now we can fit a Bayesian model that takes this mvnormal distribution (likelihood) for our data/predictions
* and combines with shrinkage priors based on ridge regression  
* this one-sample Bayesian analysis then gives a posterior distribution, which we can draw from to generate 
* simulated models (but avoiding the fully sim-based approach, so is much quicker)
 

	nois bayesmh y1 y2 y3 y4 y5 y6 y7 y8 y9 y10 y11 =,  likelihood(mvnormal(varcov_new))  ///
	prior({y11:_cons} , normal(0, 1e6))  ///
 prior( {y1:_cons} {y2:_cons} {y3:_cons} ///
  {y4:_cons}  {y5:_cons}  {y6:_cons} ///
   {y7:_cons}  {y8:_cons}  {y9:_cons} ///
    {y10:_cons}    ,   normal(0, {lam})) /// 
 prior({lam=1},  igamma(0.01, 0.01)) /// 
 rseed(66) dots saving(simdata, replace) thinning(10) hpd ///
adaptation(every(50)) block({y1:_cons} {y2:_cons} ///
 {y3:_cons} {y4:_cons}  {y5:_cons}  {y6:_cons} ///
   {y7:_cons}  {y8:_cons}  {y9:_cons} ///
    {y10:_cons} {y11:_cons} , split) block({lam}) mcmcsize(1000) burnin(20000) /* nchains(4) */
	
frame create simdata
frame simdata: use simdata
frame change simdata

* create predictions in the evaluation data 
* by using the stored (e.g., 1000) parameter estimates from the posterior distributions of the fitted Bayesian model 
* and then applying each equation to make predictions

mkmat eq1_p1 eq2_p1 eq3_p1 eq4_p1 eq5_p1 eq6_p1 eq7_p1 eq8_p1 eq9_p1 eq10_p1 eq11_p1, matrix(parameters)

frame change eval 
gen intercept = 1
rename true_p_new p

mat param = parameters'
mata: parametersmata = st_matrix("param")

local mata_var_list "ageyrssd  ga_diag1sd  hist2sd  hist3sd  pcr1sd  serurea11sd  pcsd  sbpsd  trt_ahsd  trt_mgso4sd  intercept"

mata: st_view(D=., ., "`mata_var_list'")

mata: p = invlogit(D*parametersmata)

forvalues j = 1/1000 {
	
mata: st_view(l`j'=., ., st_addvar("double", "p_bs`j'"))
mata: l`j'[., 1] = p[.,`j''] 

mata: st_view(lp`j'=., ., st_addvar("double", "lp`j'"))
mata: lp`j'[., 1] = D * parametersmata[.,`j'']

}

frame put p p_bs*, into(predictions)
frame put p lp*, into(lin_predictors)

* create matrix to store performance measures
matrix performance = J(1000,5,.)


qui forvalues i = 1/1000 {

	
* list simulation number in matrix of performance measures
matrix performance[`i',1] = `i'

nois _dots `i' 0

* c-statistic
pmcstat p_bs`i' outcome
matrix performance[`i',2] = r(cstat)

* calibration slope
logistic outcome lp`i', coef
matrix performance[`i',3] = _b[lp`i']

*** Net benefit
* set threshold
scalar z = 0.5
* NB of using the model (obtained by averaging over individuals)
gen model=(p_bs`i'>z)*(p-(1-p)*z/(1-z)) 
qui sum model
matrix performance[`i',4] = r(mean)
drop model

* MAPE
gen abs_p = abs(p_bs`i'-p)
qui sum abs_p
matrix performance[`i',5]  = r(sum)/_N

drop abs_p
	
}




*** calculate uncertainty in model predictions (95% intervals of predictions)

frame change predictions

keep p p_bs*

* empirical CI width for prediction instability 
egen lower_ci = rowpctile(p_bs*), p(2.5)
egen upper_ci = rowpctile(p_bs*), p(97.5)
gen width = upper_ci - lower_ci
nois summ width
nois centile width, centile(2.5 50 97.5)


* Net benefit
gen all = p-(1-p)*z/(1-z)
sum all
local ENBall = r(mean) 

*NB of using the correct risks
gen correct = (p>z)*(p-(1-p)*z/(1-z))
sum correct
local ENBmax = r(mean)


*** summarise performance
*rename columns of the matrix 
mat colnames performance = sim_n cstat cslope NBmodel MAPE 

frame create perf
frame perf: svmat performance, n(col)

frame change perf


* csal lope 
nois summ cslope, 
nois centile cslope, centile(2.5 50 97.5)

gen slope_degrad = cslope - 1
nois summ slope_degrad
nois centile slope_degrad, centile(2.5 50 97.5)

* prob cal slope between 0.9 and 1.10
gen yes1 = 0
replace yes1 = 1 if cslope >= 0.9 & cslope <= 1.1
nois  sum yes1
nois disp "P(0.9 < slope < 1.1)" " = " r(mean)

* prob cal slope between 0.85 and 1.15
gen yes2 = 0
replace yes2 = 1 if cslope >= 0.85 & cslope <= 1.15
nois  sum yes2
nois disp "P(0.85 < slope < 1.15)" " = " r(mean)

* cstat
nois summ cstat
nois centile cstat, centile(2.5 50 97.5)
* true model cstat is 0.759 in eval data, so work out degradation 
gen cstat_degrad = cstat - 0.759
nois summ cstat_degrad
nois centile cstat_degrad, centile(2.5 50 97.5)


* mape
nois summ MAPE
nois centile MAPE, centile(2.5 50 97.5)


* NB based on a threshold of 0.5

* net benefit of the developed model
nois  sum NBmodel
nois  centile NBmodel, centile(2.5 50 97.5)
 
* degradation of developed model (true model net benefit is 0.407 )
 gen NBmodel_degrad = NBmodel - 0.407
nois  summ NBmodel_degrad
nois  centile NBmodel_degrad, centile(2.5 50 97.5)
 
 gen NBmodel_degrad_percent = (100*(NBmodel/0.407))
nois summ NBmodel_degrad_percent
nois centile NBmodel_degrad_percent, centile(2.5 50 97.5)

* model's assurance probability based on acheiving at least 90% of the true model NB
gen NBmodel_yes = 1
replace NBmodel_yes = 0 if NBmodel_degrad_percent < 90
nois summ NBmodel_yes
nois disp "P(NB_model >=90% of NB true model)" " = " r(mean)
 
 
 timer off 1
nois timer list 

}

****** END *****





